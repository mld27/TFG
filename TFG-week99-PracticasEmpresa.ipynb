{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e8d683",
   "metadata": {},
   "source": [
    "# 1 - DATA PROCESS\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from io import StringIO\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.cm as cm\n",
    "import shutil\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c6aa5",
   "metadata": {},
   "source": [
    "---------\n",
    "## Funci√≥n `normalize(file_paths, capitalize_c=False)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Convierte los archivos de texto a un formato uniforme, reemplazando comas por puntos decimales. Adem√°s, puede renombrar archivos cuyo nombre empiece por \"c\" para que empiecen por \"C\".  \n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `file_paths`: lista de rutas de archivos a procesar.  \n",
    "- `capitalize_c`: booleano que controla si se renombran los archivos que empiezan con \"c\".  \n",
    "- Variables internas como `dirname`, `basename`, `new_basename`, `new_path` se usan para manejar rutas y renombrar.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `file_paths`: lista de strings con rutas de archivos `.txt`.  \n",
    "- `capitalize_c`: opcional (por defecto `False`), si se activa renombra los archivos.  \n",
    "\n",
    "**Outputs:**  \n",
    "- No devuelve nada directamente.  \n",
    "- Modifica los archivos en disco (reemplaza comas por puntos y, opcionalmente, renombra).  \n",
    "- Muestra mensajes en consola con el progreso o errores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78326cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(file_paths, capitalize_c = False):\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f: #cambia de comas a puntos\n",
    "                content = f.read()\n",
    "            updated = content.replace(',', '.')\n",
    "\n",
    "            with open(path, 'w', encoding='utf-8') as f:\n",
    "                f.write(updated)\n",
    "            \n",
    "            \n",
    "            #print(f\"Updated in place: {path}\")\n",
    "            \n",
    "            if capitalize_c:\n",
    "                dirname = os.path.dirname(path)\n",
    "                basename = os.path.basename(path)\n",
    "                \n",
    "                if basename.startswith('c'):\n",
    "                    new_basename = 'C' + basename[1:]\n",
    "                    new_path = os.path.join(dirname, new_basename)\n",
    "                    \n",
    "                    os.rename(path, new_path)\n",
    "                    print(f\"Renamed file: {path} ‚Üí {new_path}\")\n",
    "                    path = new_path  # Update path for future processing\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25dee7d",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Funci√≥n `thresholds(filepath)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Lee de un archivo de texto los valores de Upper y Lower Threshold.  \n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `filepath`: ruta del archivo.  \n",
    "- `upper`, `lower`: se inicializan en `None` y se actualizan al leer las l√≠neas correspondientes.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `filepath`: string con la ruta del archivo `.txt`.  \n",
    "\n",
    "**Outputs:**  \n",
    "- Una tupla `(upper, lower)` con los umbrales en formato `float`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholds(filepath):\n",
    "    upper, lower = None, None\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Upper Threshold:' in line:\n",
    "                upper = float(line.split('\\t')[1].strip())\n",
    "            elif 'Lower Threshold:' in line:\n",
    "                lower = float(line.split('\\t')[1].strip())\n",
    "    return upper, lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6815d1",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Funci√≥n `clean_data(filepath, delimiter='\\t')`\n",
    "\n",
    "**Finalidad:**  \n",
    "Carga los datos de un archivo de texto, limpiando l√≠neas vac√≠as y descartando las no num√©ricas. Devuelve un DataFrame con las columnas de tiempo y amplitud.  \n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `filepath`: ruta al archivo.  \n",
    "- `delimiter`: separador usado en el archivo (por defecto tabulador).  \n",
    "- `rows`: lista acumuladora de los pares [tiempo, amplitud].  \n",
    "\n",
    "**Inputs:**  \n",
    "- `filepath`: ruta del archivo de datos.  \n",
    "- `delimiter`: opcional, separador de columnas (default: tabulador `\\t`).  \n",
    "\n",
    "**Outputs:**  \n",
    "- DataFrame de pandas con columnas `['Time', 'Amplitude']`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filepath, delimiter='\\t'):\n",
    "    rows = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(delimiter)\n",
    "            while len(parts) < 2:\n",
    "                parts.append('')\n",
    "\n",
    "            try:\n",
    "                time_val = float(parts[0].strip())\n",
    "                amplitude_val = float(parts[1].strip())\n",
    "                rows.append([time_val, amplitude_val])\n",
    "            except ValueError:\n",
    "                continue  # Skip non-numeric lines\n",
    "\n",
    "    return pd.DataFrame(rows, columns=['Time', 'Amplitude'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1009f66",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Funci√≥n `extract_amplitude(df)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Extrae un segmento del DataFrame correspondiente a un ciclo de amplitud, delimitado entre la segunda y tercera aparici√≥n del valor de tiempo 0.000. Este segmento corresponde al eje Anterior-Posterior de la respiraci√≥n del paciente practicando el DIBH.  \n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `df`: DataFrame de entrada.  \n",
    "- `zero_indices`: posiciones donde `Time ‚âà 0`.  \n",
    "- `start`, `end`: √≠ndices que marcan los l√≠mites del segmento.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `df`: DataFrame con columnas `['Time','Amplitude']`.  \n",
    "\n",
    "**Outputs:**  \n",
    "- Sub-DataFrame con el rango de inter√©s, reindexado.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amplitude(df):\n",
    "    # Find indices where Time is exactly (or very close to) 0.000\n",
    "    zero_indices = df.index[abs(df['Time']) < 1e-6].tolist()\n",
    "    \n",
    "    # Ensure there are at least 3 zero crossings\n",
    "    if len(zero_indices) < 3:\n",
    "        raise ValueError(\"Dataframe doesn't contain at least three 0.000 entries in 'Time' column\")\n",
    "    \n",
    "    start = zero_indices[1]\n",
    "    end = zero_indices[2]\n",
    "    \n",
    "    # Extract and return the segment\n",
    "    return df.iloc[start:end].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68129a15",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Funci√≥n `extract_activation_data(filepaths=None)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Extrae los eventos de activaci√≥n del haz (\"Beam Enable/Disable Moments\") de uno o varios archivos y los devuelve en un DataFrame.  \n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `filepaths`: lista de rutas a archivos.  \n",
    "- `activation_rows`: lista con pares [tiempo, valor de beam, archivo origen].  \n",
    "- `in_activation_section`: bandera para saber si se est√° leyendo la secci√≥n correcta.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `filepaths`: lista de archivos, string con un archivo, o `None` (si es None busca `Camp*.txt`).  \n",
    "\n",
    "**Outputs:**  \n",
    "- DataFrame con columnas `['Time','Beam','Source_File']`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activation_data(filepaths=None):\n",
    "    if filepaths is None:\n",
    "        filepaths = sorted(glob.glob('Camp*.txt'))\n",
    "    elif isinstance(filepaths, str):\n",
    "        filepaths = [filepaths]\n",
    "    if len(filepaths) < 1:\n",
    "        raise ValueError(\"Need at least 1 file to extract activation data\")\n",
    "\n",
    "    activation_rows = []\n",
    "    for filepath in filepaths:\n",
    "        in_activation_section = False\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if \"Beam Enable/Disable Moments\" in line:\n",
    "                    in_activation_section = True\n",
    "                    continue\n",
    "                if in_activation_section and line:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        try:\n",
    "                            time_val = float(parts[0].strip())\n",
    "                            activation_val = int(float(parts[1].strip()))  # robust: acepta 1 o 1.0\n",
    "                            activation_rows.append([time_val, activation_val, filepath])\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "    return pd.DataFrame(activation_rows, columns=['Time', 'Beam', 'Source_File'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a0e1d",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Funci√≥n `check_overlap(df1, df2, tol=0.005, window=10, min_matches=300)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Detecta si hay solapamiento entre el final de `df1` y el inicio de `df2`, comparando amplitudes en una ventana de tiempo.\n",
    "\n",
    "La l√≥gica de la funci√≥n es, ya que sabemos que siempre que se solapen, lo hacen alrededor de 8s del final de la `df1` con 8s del principio de la `df2`, usamos la amplitud del `idx=0` de la `df2` para ir mirando cada valor desde los ultimos `window` segundos para ver si hay algun punto en el que las amplitudes coincidan con una diferencia m√°xima de `tol`. Si coinciden en el indice `idx'=i` de la df1, se hace `+=1` a `matches` y se compara el `idx=1` de la `df2` con el `idx'=i+1` de la `df1` y asi sucesivamente hasta el ultimo indice posible de `df1`. Si al finalizar hay mas de `min_matches` coincidencias, se determina que si que hay overlap, sin√≥, suponemos que las coincidencias han sido aleatorias y no hay overlap.\n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `df1`, `df2`: DataFrames con datos de amplitud extraidos con las funciones clean_data i extract_amplitude.\n",
    "- `tol`: tolerancia en amplitud (cm) a partir de la cual se determina un match.\n",
    "- `window`: tama√±o de la ventana (s) en la que puede aparecer el overlap.\n",
    "- `min_matches`: n√∫mero m√≠nimo de coincidencias para determinat si hay overlap.\n",
    "- Variables intermedias: `t1`, `a1`, `t2`, `a2`, `a1_window`.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `df1`, `df2`: DataFrames con columnas `['Time','Amplitude']`.  \n",
    "- `tol`: tolerancia de coincidencia (default 0.005).  \n",
    "- `window`: segundos a revisar (default 10).  \n",
    "- `min_matches`: m√≠nimo de coincidencias (default 300).  \n",
    "\n",
    "**Outputs:**  \n",
    "- Tupla `(True, idx)` si hay solapamiento, indicando el √≠ndice de inicio de solapamiento en `df1`.  \n",
    "- Tupla `(False, None)` si no se encuentra solapamiento.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d3d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(df1, df2, tol=0.005, window=10, min_matches=300):\n",
    "\n",
    "    #dataframes con los valores de ambos df separados\n",
    "    t1 = df1['Time'].values\n",
    "    a1 = df1['Amplitude'].values\n",
    "    t2 = df2['Time'].values\n",
    "    a2 = df2['Amplitude'].values\n",
    "\n",
    "    t_max = t1[-1] #coge el √∫ltimo indice del df1: el ultimo dato tomado en el df1\n",
    "    mask = t1 >= t_max - window #nuevo df en el que solo van a estar los indices que corresponden a los ultimos 'window' segundos\n",
    "    a1_window = a1[mask] #acortamos a_1 a solo los valores de los ultimos 10 segundos de df1\n",
    "\n",
    "    # buscar coincidencia entre final de df1 y comienzo de df2\n",
    "    for start in range(len(a1_window)):\n",
    "        length = min(len(a1_window) - start, len(a2))\n",
    "        diffs = np.abs(a1_window[start:start+length] - a2[:length])\n",
    "        matches = np.sum(diffs <= tol)\n",
    "        if matches >= min_matches: #si hay mas de 300 matches, se considera que hay solapamiento\n",
    "            idx_start = np.where(mask)[0][start]\n",
    "            return True, idx_start\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431ee59",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Funci√≥n `build_df_with_beam(filepath)`\n",
    "\n",
    "**Finalidad:**  \n",
    "El objetivo de esta funci√≥n es juntar en un √∫nico Dataframe los dataframes de amplitud i del beam.\n",
    "\n",
    "La l√≥gica de la funci√≥n es primeramente crear los dataframes con las funciones `extract_amplitude` y `extract_activation_data`.\n",
    "Juntamos en una Serie todos los tiempos (los de ambos dataframes) para a√±adirla a un Dataframe de 3 columnas: `['Time','Amplitude','Beam']`. \n",
    "\n",
    "El `df_amp` tiene muestras en tiempos cada 0.015s, mientras que `df_beam` tiene unicamente muestras cuando detecta que puede haber activaci√≥n o desactivaci√≥n. Por tanto, las muestras es practicamente imposible que coincidan en tiempo. Esto produce que, cuando se a√±adan las columnas del `df_amp` i del `df_beam` al `df_total`, haya un `NaN` en la columna de las amplitudes para los tiempos de activaci√≥n y miles de `NaN` en la columna de beam para los tiempos que no son de activaci√≥n. \\\n",
    "Para solucionarlo, simplemente hacemos un `fill forward` con una funci√≥n de pandas para que, si detecta un `NaN`, copie el valor anterior no nulo en ese indice. Si hacemos este proceso para ambas columnas, conseguimos rellenar todo el dataframe de valores. \n",
    "\n",
    "Hay un caso particular que hay que tener en cuenta a veces. No siempre hay un valor de activaci√≥n para `t=0`, por lo que se definen como `NaN` todos los valores de activaci√≥n hasta que hay una. Para solucionarlo, se hace un `fill backwards` con el valor opuesto al primer `no-NaN` que se encuentre en la columna de activaci√≥n: si el primer valor es un 1, rellena el dataframe hacia atras con 0, y viceversa.\n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `df_amp`: amplitud extra√≠da con `extract_amplitude`.  \n",
    "- `df_beam`: datos de beam extra√≠dos con `extract_activation_data`.  \n",
    "- `df_total`: DataFrame combinado y rellenado.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `filepath`: ruta del archivo `.txt`.  \n",
    "\n",
    "**Outputs:**  \n",
    "- DataFrame conjunto con columnas `['Time','Amplitude','Beam']`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_with_beam(filepath):\n",
    "\n",
    "    # --- Amplitud ---\n",
    "    df_amp = extract_amplitude(clean_data(filepath))\n",
    "\n",
    "    # --- Eventos Beam crudos ---\n",
    "    df_beam = extract_activation_data(filepath)[[\"Time\", \"Beam\"]].sort_values(\"Time\")\n",
    "    \n",
    "    # 1. Unir todos los tiempos y ordenarlos\n",
    "    all_times = pd.Series(sorted(set(df_amp[\"Time\"]).union(set(df_beam[\"Time\"]))), name=\"Time\")\n",
    "    \n",
    "    # 2. Crear dataframe base\n",
    "    df_total = pd.DataFrame(all_times)\n",
    "    df_total[\"Amplitude\"] = pd.NA\n",
    "    df_total[\"Beam\"] = pd.NA\n",
    "\n",
    "    # 3. Insertar valores conocidos de Amplitude\n",
    "    df_total = df_total.merge(df_amp, on=\"Time\", how=\"left\", suffixes=(\"\", \"_amp\"))\n",
    "    df_total[\"Amplitude\"] = df_total[\"Amplitude\"].combine_first(df_total[\"Amplitude_amp\"])\n",
    "    df_total = df_total.drop(columns=[\"Amplitude_amp\"])\n",
    "\n",
    "    # 4. Insertar valores conocidos de Beam\n",
    "    df_total = df_total.merge(df_beam, on=\"Time\", how=\"left\", suffixes=(\"\", \"_beam\"))\n",
    "    df_total[\"Beam\"] = df_total[\"Beam\"].combine_first(df_total[\"Beam_beam\"])\n",
    "    df_total = df_total.drop(columns=[\"Beam_beam\"])\n",
    "\n",
    "    # 5. Rellenar hacia adelante\n",
    "    df_total = df_total.sort_values(\"Time\").reset_index(drop=True)\n",
    "    df_total[\"Amplitude\"] = df_total[\"Amplitude\"].ffill()\n",
    "    df_total[\"Beam\"] = df_total[\"Beam\"].ffill()\n",
    "    \n",
    "    # 6. Manejo especial del primer valor de Beam si es NaN\n",
    "    if pd.isna(df_total.loc[0, \"Beam\"]):\n",
    "        first_valid = df_total[\"Beam\"].first_valid_index()\n",
    "        if first_valid is not None:\n",
    "            first_val = df_total.loc[first_valid, \"Beam\"]\n",
    "            if first_val == 1:\n",
    "                # Si el primer valor v√°lido es 1, rellenamos con 0 hacia atr√°s\n",
    "                df_total.loc[:first_valid, \"Beam\"] = df_total[\"Beam\"].bfill()\n",
    "                df_total.loc[:first_valid-1, \"Beam\"] = 0\n",
    "            elif first_val == 0:\n",
    "                # Si el primer valor v√°lido es 0, rellenamos con 1 hacia atr√°s\n",
    "                df_total.loc[:first_valid, \"Beam\"] = df_total[\"Beam\"].bfill()\n",
    "                df_total.loc[:first_valid-1, \"Beam\"] = 1\n",
    "\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f7cb6",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Funci√≥n `merge_timelines(files, tol=0.005, window=10, min_matches=300)`\n",
    "\n",
    "**Finalidad:**  \n",
    "El objetivo de esta funci√≥n es concatenar todos los archivos de una misma sessi√≥n en un unico archivo, segregando los caso de overlap y los de no y guardando los valores de threshold. Todo en un unico archivo.\n",
    "\n",
    "La l√≥gica de la funci√≥n es crear un dataframe vaci√≥ e ir a√±adiendo los dataframes de cada archivo a este conjunto.\n",
    "\n",
    "Primero se a√±adiran en una fila de indice 0 los valores de upper i lower threshold para poder guardarlos.\n",
    "\n",
    "A partir de ahi, la idea es a√±adir el primer dataframe y pasar la funci√≥n `check_overlap`. Si hay overlap, se eliminan los datos del primer archivo desde el overlap hasta el final y se calcula el `timeshift` que se debe sumar al segundo archivo para que la linea temporal empalme perfectamente, asi sucesivamente hasta empalmarlos todos. Si noo hay overlap, como los archivos ya empalman de por si, simplemente es sumarle el tiempo del ultimo indice del primer dataframe al segundo, y asi crear una linea temporal continua.\n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `files`: lista de rutas a archivos.  \n",
    "- `df_total`: acumulador de los DataFrames concatenados.  \n",
    "- `upper, lower`: thresholds le√≠dos del primer archivo.  \n",
    "- `meta_row`: primera fila con los thresholds.  \n",
    "- Variables intermedias: `overlap`, `idx`, `time_shift`, `df_new_shifted`.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `files`: lista de rutas de archivos `.txt`.  \n",
    "- `tol`, `window`, `min_matches`: par√°metros de detecci√≥n de solapamiento.  \n",
    "\n",
    "**Outputs:**  \n",
    "- DataFrame final con columnas `['Time','Amplitude','Beam','UpperThreshold','LowerThreshold']`, incluyendo la fila inicial con thresholds.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_timelines(files, tol=0.005, window=10, min_matches=300):\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No se encontraron archivos .txt en la carpeta\")\n",
    "\n",
    "    normalize(files)\n",
    "    df_total = pd.DataFrame(columns=[\"Time\", \"Amplitude\", \"Beam\",\"UpperThreshold\",\"LowerThreshold\"])\n",
    "    upper, lower = thresholds(files[0])\n",
    "    meta_row = pd.DataFrame([{\n",
    "    \"Time\": None,\n",
    "    \"Amplitude\": None,\n",
    "    \"Beam\": None,\n",
    "    \"UpperThreshold\": upper,\n",
    "    \"LowerThreshold\": lower\n",
    "    }])\n",
    "\n",
    "    for i, f in enumerate(files):\n",
    "        df_new = build_df_with_beam(f)\n",
    "\n",
    "        if df_total.empty:\n",
    "            df_total = df_new.copy()\n",
    "            continue\n",
    "\n",
    "        # comprobamos solapamiento\n",
    "        overlap, idx = check_overlap(df_total, df_new, tol=tol, window=window, min_matches=min_matches)\n",
    "\n",
    "        if overlap:\n",
    "            # tiempo de inicio del solapamiento en df_total\n",
    "            t_overlap = df_total.iloc[idx][\"Time\"]\n",
    "            # inicio del nuevo archivo\n",
    "            t_new_start = df_new[\"Time\"].iloc[0]\n",
    "            # calculamos shift para alinear inicio del nuevo con el inicio de solapamiento\n",
    "            time_shift = t_overlap - t_new_start\n",
    "\n",
    "            df_new_shifted = df_new.copy()\n",
    "            df_new_shifted[\"Time\"] = df_new[\"Time\"] + time_shift\n",
    "\n",
    "            #print(f\"‚úÖ Overlap detectado en {f} con inicio en t = {t_overlap:.3f} s (shift={time_shift:.6f})\")\n",
    "\n",
    "            # reemplazamos desde idx en adelante por el nuevo archivo\n",
    "            df_total = pd.concat([df_total.iloc[:idx], df_new_shifted], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            # si no hay overlap, el archivo nuevo empieza despu√©s del √∫ltimo tiempo\n",
    "            last_time = df_total[\"Time\"].iloc[-1]\n",
    "            dt = np.median(df_new[\"Time\"].diff().dropna())\n",
    "            if not np.isfinite(dt) or dt <= 0:\n",
    "                dt = 1e-6\n",
    "            time_shift = last_time - df_new[\"Time\"].iloc[0] + dt\n",
    "\n",
    "            df_new_shifted = df_new.copy()\n",
    "            df_new_shifted[\"Time\"] = df_new[\"Time\"] + time_shift\n",
    "\n",
    "            #print(f\"‚ùå No overlap detectado en {f}. Shift aplicado = {time_shift:.6f}\")\n",
    "\n",
    "            df_total = pd.concat([df_total, df_new_shifted], ignore_index=True)\n",
    "\n",
    "    # opcional: redondear tiempos para evitar problemas de floats (si quieres mantenerlo)\n",
    "    df_total[\"Time\"] = df_total[\"Time\"].round(6)\n",
    "    df_final = pd.concat([meta_row, df_total], ignore_index=True)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57edd297",
   "metadata": {},
   "source": [
    "--------\n",
    "## Funci√≥n `cut_delay(df)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Recorta el primer segundo de cada activaci√≥n del `Beam`.  \n",
    "En los datos de DIBH, cuando la columna `Beam` pasa de `0` a `1`, significa que el haz de radiaci√≥n puede habilitarse, pero en la pr√°ctica no se activa hasta **1 segundo despu√©s**. Esta funci√≥n corrige ese desfase: en cada transici√≥n 0‚Üí1, pone la columna `Beam=0` desde el instante de transici√≥n `t` hasta `t+1`.  \n",
    "\n",
    "**Variables utilizadas:**  \n",
    "- `df`: DataFrame extraido de la funci√≥n `merge_timelines`.\n",
    "- `times`: array con los valores de tiempo (`df['Time'].values`).  \n",
    "- `beam`: array con los valores de la columna `Beam`.  \n",
    "- `transitions`: array booleano que detecta las posiciones en que `Beam` pasa de 0 a 1.  \n",
    "- `transition_indices`: √≠ndices de las transiciones 0‚Üí1.  \n",
    "- `mask`: m√°scara booleana que selecciona las filas entre `t` y `t+1s` despu√©s de cada transici√≥n.  \n",
    "\n",
    "**Inputs:**  \n",
    "- `df`: DataFrame con las columnas `['Time','Amplitude','Beam','UpperThreshold','LowerThreshold']`.  \n",
    "\n",
    "**Outputs:**  \n",
    "- `df_mod`: DataFrame modificado, id√©ntico al original salvo que en la columna `Beam` se corrige el retraso de 1s despu√©s de cada transici√≥n 0‚Üí1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f970ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_delay(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df_mod = df.copy()\n",
    "    times = df_mod[\"Time\"].values\n",
    "    beam = df_mod[\"Beam\"].values\n",
    "\n",
    "    # Detectar transiciones 0 -> 1\n",
    "    transitions = (beam[1:] == 1) & (beam[:-1] == 0)\n",
    "    transition_indices = transitions.nonzero()[0] + 1  # desplazamos +1 porque es la segunda muestra la que pasa a 1\n",
    "\n",
    "    for idx in transition_indices:\n",
    "        t0 = times[idx]\n",
    "        # marcar como 0 todas las filas con time < t0+1\n",
    "        mask = (times >= t0) & (times < t0 + 1)\n",
    "        df_mod.loc[mask, \"Beam\"] = 0\n",
    "\n",
    "    return df_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777143f9",
   "metadata": {},
   "source": [
    "---\n",
    "## Funci√≥n `import_UM(file_path)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Esta funci√≥n importa un archivo `.txt` que contiene los valores de **Unidades Monitor (UM)** de un paciente, generado previamente por la funci√≥n `import_queries`.  \n",
    "Cada l√≠nea del archivo corresponde a un campo de irradiaci√≥n distinto.  \n",
    "A partir de estos valores, la funci√≥n calcula el tiempo m√≠nimo de irradiaci√≥n por campo (`t_camp = UM / 10`) y devuelve un `DataFrame` con el formato est√°ndar utilizado en el procesamiento de sesiones DIBH.\n",
    "\n",
    "\n",
    "**Entradas:**  \n",
    "- `file_path` ‚Üí Ruta completa del archivo `.txt` que contiene las Unidades Monitor del paciente.  \n",
    "  Ejemplo:  r\"C:\\Users\\Mario\\Desktop\\Fisica\\TFG Clinic\\Mario\\DIBH\\Queries\\4806222\\4806222_UM.txt\"\n",
    "  \n",
    "\n",
    "**Formato del archivo esperado:**  \n",
    "El archivo debe contener **una lista de valores de UM**, uno por l√≠nea, sin encabezados ni separadores adicionales.  \n",
    "Ejemplo de contenido:\n",
    "\n",
    "150\n",
    "200\n",
    "180\n",
    "\n",
    "Cada n√∫mero representa las Unidades Monitor (UM) administradas en un campo del tratamiento.\n",
    "\n",
    "\n",
    "**Variables internas:**  \n",
    "- `um_values` ‚Üí Lista de valores num√©ricos le√≠dos desde el archivo (convertidos a tipo `float`).  \n",
    "- `t_camp` ‚Üí Lista con el tiempo m√≠nimo de irradiaci√≥n de cada campo, calculado como `UM / 10`.  \n",
    "- `df_UM` ‚Üí DataFrame resultante con dos columnas:  \n",
    "  | Columna | Descripci√≥n |  \n",
    "  |----------|--------------|  \n",
    "  | `UM` | Unidades Monitor de cada campo |  \n",
    "  | `t_camp` | Tiempo m√≠nimo de irradiaci√≥n (UM / 10) |\n",
    "\n",
    "\n",
    "**L√≥gica del c√≥digo:**  \n",
    "1. Se abre el archivo `.txt` y se leen todas las l√≠neas con valores num√©ricos.  \n",
    "2. Se eliminan posibles l√≠neas vac√≠as y se convierten los valores a `float`.  \n",
    "3. Se calcula el tiempo m√≠nimo (`t_camp`) dividiendo cada valor de `UM` entre 10 (dado que la tasa de dosis es de **10 UM/s**).  \n",
    "4. Se construye un `DataFrame` con las columnas `'UM'` y `'t_camp'`.  \n",
    "5. Se devuelve el `DataFrame` para ser utilizado en la segmentaci√≥n de campos (`detect_groups`).\n",
    "\n",
    "\n",
    "**Salida:**  \n",
    "- `df_UM` ‚Üí `DataFrame` con las columnas:  \n",
    "  | UM | t_camp |  \n",
    "  |----|---------|  \n",
    "  | 150.0 | 15.0 |  \n",
    "  | 200.0 | 20.0 |  \n",
    "  | 180.0 | 18.0 |  \n",
    "\n",
    "\n",
    "**Uso posterior:**\n",
    "El DataFrame obtenido (df_UM) se emplea directamente en la funci√≥n detect_groups(df_data, df_UM) para segmentar los datos de la sesi√≥n seg√∫n los tiempos m√≠nimos de irradiaci√≥n definidos por las Unidades Monitor de cada campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_UM(file_path):\n",
    "\n",
    "    # Leer las UM (una por l√≠nea)\n",
    "    with open(file_path, 'r') as f:\n",
    "        um_values = [float(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "    # Crear DataFrame\n",
    "    df_UM = pd.DataFrame({\n",
    "        'UM': um_values,\n",
    "        't_camp': [um / 10 for um in um_values]\n",
    "    })\n",
    "\n",
    "    return df_UM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea08a27",
   "metadata": {},
   "source": [
    "---\n",
    "## Funci√≥n `detect_groups(df_data, df_UM, lookahead_s=5.0, verbose=True)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Esta funci√≥n identifica y etiqueta los distintos **campos (grupos)** de un tratamiento DIBH dentro del registro temporal de la sesi√≥n, bas√°ndose en los intervalos en los que el haz (`Beam`) est√° activo y en los tiempos m√≠nimos por campo (`t_camp`) calculados a partir de las Unidades Monitor (UM).  \n",
    "Incluye adem√°s un **modo de depuraci√≥n** (`verbose=True`) que imprime en consola los intervalos de tiempo asignados a cada grupo, permitiendo comprobar visualmente si la segmentaci√≥n es coherente con la realidad del tratamiento.\n",
    "\n",
    "\n",
    "**Entradas:**  \n",
    "- `df_data` ‚Üí `DataFrame` con las columnas:  \n",
    "  `['Time', 'Amplitude', 'Beam', 'UpperThreshold', 'LowerThreshold']`  \n",
    "  (Resultado del procesamiento tras aplicar `merge_timelines` y `cut_delay`).\n",
    "\n",
    "- `df_UM` ‚Üí `DataFrame` con las columnas:  \n",
    "  `['UM', 't_camp']`, donde `t_camp` es el tiempo m√≠nimo de irradiaci√≥n para cada campo.\n",
    "\n",
    "- `lookahead_s` ‚Üí *(float, opcional, por defecto 5.0)*  \n",
    "  Ventana temporal (en segundos) que permite incluir activaciones posteriores cercanas dentro del mismo campo.  \n",
    "  Esto evita dividir un mismo campo si se produce una breve salida de los m√°rgenes de respiraci√≥n.\n",
    "\n",
    "- `verbose` ‚Üí *(bool, opcional, por defecto True)*  \n",
    "  Si est√° activado, la funci√≥n imprime por consola un resumen de cada grupo detectado:\n",
    "  - N√∫mero de grupo  \n",
    "  - Tiempo de inicio (`start_time`) y fin (`end_time`)  \n",
    "  - Duraci√≥n total del campo  \n",
    "  - Si se aplic√≥ el criterio de *lookahead* o si el campo fue incompleto por fin de datos  \n",
    "\n",
    "\n",
    "**L√≥gica del algoritmo:**  \n",
    "1. **Identificaci√≥n de intervalos activos:**  \n",
    "   Se buscan los tramos consecutivos donde `Beam == 1` (inicio y fin de cada activaci√≥n).  \n",
    "\n",
    "2. **Acumulaci√≥n por campo:**  \n",
    "   Se van sumando las duraciones de cada activaci√≥n hasta alcanzar el tiempo m√≠nimo de irradiaci√≥n (`t_camp`) del campo actual.  \n",
    "3. **Agrupaci√≥n inteligente:**  \n",
    "   Si tras alcanzar `t_camp` se detecta otra activaci√≥n dentro de los siguientes `lookahead_s` segundos, se considera que **pertenece al mismo campo** (para evitar falsos cambios).  \n",
    "\n",
    "4. **Fin de datos:**  \n",
    "   Si se est√° procesando el √∫ltimo campo y el archivo termina antes de alcanzar su `t_camp`, se asigna el campo hasta la √∫ltima desactivaci√≥n (`Beam` pasa de 1 a 0).  \n",
    "\n",
    "5. **Asignaci√≥n de grupos:**  \n",
    "   A cada muestra se le asigna el n√∫mero de grupo correspondiente (1, 2, 3, ‚Ä¶).  \n",
    "   Los puntos no asignados quedan con valor `0` en la columna `Grupo`.  \n",
    "\n",
    "6. **Mensajes de depuraci√≥n (`verbose=True`):**  \n",
    "   Por cada campo detectado, se muestra una l√≠nea en consola como esta:\n",
    "   üü¢ Grupo 1: 12.00s ‚Üí 45.20s (duraci√≥n = 33.20s, lookahead aplicado: True)\n",
    "   üü° √öltimo grupo (3): datos terminados antes de t_camp. Asigna hasta 123.00s\n",
    "   \n",
    "---\n",
    "\n",
    "   **Salidas:**  \n",
    "    - `DataFrame` con las columnas:  \n",
    "    | Columna | Descripci√≥n |  \n",
    "    |----------|--------------|  \n",
    "    | `Time` | Tiempo de adquisici√≥n de cada muestra |  \n",
    "    | `Amplitude` | Se√±al respiratoria medida |  \n",
    "    | `Beam` | Estado del haz (1 = activo, 0 = inactivo) |  \n",
    "    | `Grupo` | Campo asignado (1, 2, 3, ‚Ä¶) o 0 si no pertenece a ning√∫n campo |  \n",
    "    | `UpperThreshold` | L√≠mite superior del rango de inspiraci√≥n |  \n",
    "    | `LowerThreshold` | L√≠mite inferior del rango de inspiraci√≥n |  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457da17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_groups(df_data, df_UM, lookahead_s=3.0, verbose=True):\n",
    "\n",
    "    df = df_data.copy().reset_index(drop=True)\n",
    "    df['Grupo'] = 0\n",
    "\n",
    "    if 'Time' not in df.columns or 'Beam' not in df.columns:\n",
    "        raise ValueError(\"df_data debe contener las columnas 'Time' y 'Beam'\")\n",
    "\n",
    "    times = df['Time'].to_numpy(dtype=float)\n",
    "    beam = df['Beam'].to_numpy(dtype=int)\n",
    "    n_rows = len(df)\n",
    "\n",
    "    # --- Detectar intervalos donde beam == 1 (start_idx, end_idx) inclusive ---\n",
    "    intervals = []\n",
    "    i = 0\n",
    "    while i < n_rows:\n",
    "        if beam[i] == 1:\n",
    "            start = i\n",
    "            j = i\n",
    "            while j + 1 < n_rows and beam[j + 1] == 1:\n",
    "                j += 1\n",
    "            end = j\n",
    "            intervals.append((start, end))\n",
    "            i = j + 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    if len(intervals) == 0 or len(df_UM) == 0:\n",
    "        return df[['Time', 'Amplitude', 'Beam', 'Grupo', 'UpperThreshold', 'LowerThreshold']]\n",
    "\n",
    "    n_groups = len(df_UM)\n",
    "    group_idx = 0\n",
    "    interval_ptr = 0\n",
    "\n",
    "    while interval_ptr < len(intervals) and group_idx < n_groups:\n",
    "        group_start_idx = intervals[interval_ptr][0]\n",
    "        accumulated_time = 0.0\n",
    "        last_included_end = None\n",
    "\n",
    "        while interval_ptr < len(intervals) and accumulated_time < float(df_UM.iloc[group_idx]['t_camp']):\n",
    "            s, e = intervals[interval_ptr]\n",
    "            duration = float(times[e]) - float(times[s])\n",
    "            accumulated_time += duration\n",
    "            last_included_end = e\n",
    "            interval_ptr += 1\n",
    "\n",
    "        # Caso 1: alcanzado t_camp\n",
    "        if accumulated_time >= float(df_UM.iloc[group_idx]['t_camp']):\n",
    "            # Buscar activaciones cercanas dentro de ventana lookahead_s\n",
    "            while interval_ptr < len(intervals):\n",
    "                next_start = intervals[interval_ptr][0]\n",
    "                gap = float(times[next_start]) - float(times[last_included_end])\n",
    "                if gap <= lookahead_s:\n",
    "                    last_included_end = intervals[interval_ptr][1]\n",
    "                    interval_ptr += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            df.loc[group_start_idx:last_included_end, 'Grupo'] = group_idx + 1\n",
    "\n",
    "            if verbose:\n",
    "                start_t = float(times[group_start_idx])\n",
    "                end_t = float(times[last_included_end])\n",
    "                print(f\"üü¢ Grupo {group_idx + 1}: {start_t:.2f}s ‚Üí {end_t:.2f}s \"\n",
    "                      f\"(duraci√≥n = {end_t - start_t:.2f}s, lookahead aplicado: {gap <= lookahead_s})\")\n",
    "\n",
    "            group_idx += 1\n",
    "\n",
    "        # Caso 2: √∫ltimos datos sin alcanzar t_camp\n",
    "        else:\n",
    "            if group_idx == n_groups - 1 and last_included_end is not None:\n",
    "                df.loc[group_start_idx:last_included_end, 'Grupo'] = group_idx + 1\n",
    "                if verbose:\n",
    "                    start_t = float(times[group_start_idx])\n",
    "                    end_t = float(times[last_included_end])\n",
    "                    print(f\"üü° √öltimo grupo ({group_idx + 1}): datos terminados antes de t_camp. \"\n",
    "                          f\"Asigna hasta {end_t:.2f}s (duraci√≥n = {end_t - start_t:.2f}s)\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"‚ö†Ô∏è Campo {group_idx + 1}: datos insuficientes para completar el tiempo m√≠nimo.\")\n",
    "            break\n",
    "\n",
    "    df['Grupo'] = df['Grupo'].fillna(0).astype(int)\n",
    "    return df[['Time', 'Amplitude', 'Beam', 'Grupo', 'UpperThreshold', 'LowerThreshold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20418410",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33986237",
   "metadata": {},
   "source": [
    "---\n",
    "## Funci√≥n `import_queries(file_path, output_dir=\"UM_by_ID\")`\n",
    "\n",
    "**Finalidad:**  \n",
    "Esta funci√≥n importa un archivo `.out` (separado por comas) que contiene informaci√≥n de varios pacientes y sus par√°metros de tratamiento, extrae los identificadores de paciente (`ID`) y las Unidades Monitor (`UM`), y organiza los datos creando una carpeta por paciente.  \n",
    "En cada carpeta se guarda un archivo `.txt` con los valores de `UM` correspondientes a ese paciente, para su posterior uso en el c√°lculo de los tiempos m√≠nimos de irradiaci√≥n con la funci√≥n `import_UM`.\n",
    "\n",
    "\n",
    "**Entradas:**  \n",
    "- `file_path` ‚Üí Ruta al archivo `.out` (o `.csv`) con los datos de todos los pacientes.  \n",
    "  El archivo debe tener al menos 6 columnas separadas por comas, donde:\n",
    "  - La **columna 2** contiene el **ID del paciente** (identificador √∫nico).  \n",
    "  - La **columna 6** contiene las **Unidades Monitor (UM)** del tratamiento.  \n",
    "\n",
    "- `output_dir` ‚Üí *(str, opcional)*  \n",
    "  Nombre o ruta del directorio donde se crear√°n las carpetas por paciente.  \n",
    "  Por defecto: `\"UM_by_ID\"`.\n",
    "\n",
    "\n",
    "**Variables internas:**  \n",
    "- `df` ‚Üí `DataFrame` que contiene solo las columnas seleccionadas del archivo (`ID` y `UM`).  \n",
    "- `ids_unicos` ‚Üí Lista de identificadores √∫nicos de pacientes encontrados en el archivo.  \n",
    "- `df_id` ‚Üí Subconjunto del dataframe con las filas que corresponden a un mismo paciente.  \n",
    "- `ums` ‚Üí Lista de valores de `UM` de cada paciente, redondeados al entero m√°s cercano.  \n",
    "- `folder_path` ‚Üí Ruta de la carpeta individual creada para el paciente dentro de `output_dir`.  \n",
    "- `file_out` ‚Üí Ruta completa del archivo `.txt` donde se guardan los valores `UM`.\n",
    "\n",
    "\n",
    "**L√≥gica del c√≥digo:**  \n",
    "1. Se lee el archivo `.out` usando `pandas.read_csv()` con separador por comas.  \n",
    "2. Se extraen √∫nicamente las columnas **2** e **6** (√≠ndices `[1, 5]` en base cero).  \n",
    "3. Se renombran las columnas a `ID` y `UM`.  \n",
    "4. Se redondean las UM al entero m√°s cercano (`round(0)` y `astype(int)`).  \n",
    "5. Se crea un directorio principal (`output_dir`) si no existe.  \n",
    "6. Para cada `ID` √∫nico:\n",
    "   - Se genera una carpeta con su nombre (por ejemplo `UM_by_ID/4806222/`).  \n",
    "   - Dentro de ella se guarda un archivo `.txt` con sus valores de `UM` (uno por l√≠nea).  \n",
    "7. La funci√≥n imprime por consola un resumen con el n√∫mero total de carpetas creadas.  \n",
    "8. Devuelve un diccionario con los IDs como claves y las listas de UM como valores.\n",
    "\n",
    "\n",
    "**Salida:**  \n",
    "- `dict` ‚Üí Diccionario con estructura:  \n",
    "  ```python\n",
    "  {\n",
    "      '4806222': [150, 200, 180],\n",
    "      '4810090': [120, 160],\n",
    "      ...\n",
    "  }\n",
    "\n",
    "Adem√°s, genera carpetas en el directorio `outputdir`\n",
    "\n",
    "UM_by_ID/\n",
    "\n",
    "‚îú‚îÄ‚îÄ 4806222/\n",
    "\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 4806222_UM.txt\n",
    "\n",
    "‚îú‚îÄ‚îÄ 4810090/\n",
    "\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 4810090_UM.txt\n",
    "\n",
    "‚îú‚îÄ‚îÄ ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfa64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_queries(file_path, output_dir):\n",
    "\n",
    "    # Leer archivo separado por comas\n",
    "    df = pd.read_csv(file_path, sep=',', header=None, engine='python')\n",
    "\n",
    "    # Seleccionar solo columnas 2 y 6 (√≠ndices 1 y 5 en base 0)\n",
    "    df = df.iloc[:, [1, 5]].copy()\n",
    "    df.columns = ['ID', 'UM']\n",
    "\n",
    "    # Redondear las UM al entero m√°s cercano\n",
    "    df['UM'] = df['UM'].round(0).astype(int)\n",
    "\n",
    "    # Crear directorio base si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Crear una carpeta por ID y guardar los UM en un txt\n",
    "    ids_unicos = df['ID'].unique()\n",
    "    resultado = {}\n",
    "\n",
    "    for id_paciente in ids_unicos:\n",
    "        # Filtrar las filas del paciente\n",
    "        df_id = df[df['ID'] == id_paciente]\n",
    "        ums = df_id['UM'].tolist()\n",
    "        resultado[id_paciente] = ums\n",
    "\n",
    "        # Crear carpeta del paciente\n",
    "        folder_path = os.path.join(output_dir, str(id_paciente))\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Guardar archivo .txt con los valores UM\n",
    "        file_out = os.path.join(folder_path, f\"{id_paciente}_UM.txt\")\n",
    "        with open(file_out, 'w') as f:\n",
    "            f.write('\\n'.join(map(str, ums)))\n",
    "\n",
    "    print(f\"‚úÖ Datos exportados correctamente en el directorio: '{output_dir}'\")\n",
    "    print(f\"   Se generaron {len(ids_unicos)} carpetas (una por ID de paciente).\")\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82487720",
   "metadata": {},
   "source": [
    "---\n",
    "## Funci√≥n `read_patients(path_in, path_pacientes)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Organiza autom√°ticamente los archivos `.txt` de los campos de respiraci√≥n en la estructura de carpetas de los pacientes.  \n",
    "Cada archivo tiene el formato `ID_sesion_campo.txt`, por ejemplo: `5773629_3_6.txt`, que corresponde al paciente `5773629`, sesi√≥n `3` y campo `6`.\n",
    "\n",
    "**Funcionamiento:**  \n",
    "La funci√≥n busca en la carpeta `path_in` todos los archivos `.txt` y los distribuye dentro de la carpeta `path_pacientes` en su estructura correspondiente:\n",
    "\n",
    "Pacientes/\n",
    "\n",
    "‚îú‚îÄ‚îÄ 5773629/\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ 5773629_UM.txt\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ 5773629_1/\n",
    "\n",
    "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 5773629_1_1.txt\n",
    "\n",
    "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 5773629_1_2.txt\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ 5773629_2/\n",
    "\n",
    "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ...\n",
    "\n",
    "**Inputs:**  \n",
    "- `path_in`: ruta donde est√°n los archivos `.txt` de entrada.  \n",
    "- `path_pacientes`: ruta de la carpeta principal `\"Pacientes\"` (ya generada por `import_queries`).\n",
    "\n",
    "**Outputs:**  \n",
    "No devuelve nada, pero:\n",
    "- Crea las subcarpetas de cada sesi√≥n dentro de la carpeta de cada paciente.  \n",
    "- Copia o mueve (puedes elegir) los `.txt` correspondientes a su lugar.  \n",
    "- Si un archivo no encaja con ning√∫n paciente o formato v√°lido, imprime un aviso de error indicando el nombre del archivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38685cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_patients(path_in, path_pacientes, move=False):\n",
    "\n",
    "    # Obtener lista de archivos .txt\n",
    "    all_files = [f for f in os.listdir(path_in) if f.endswith('.txt')]\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"‚ö†Ô∏è No se encontraron archivos .txt en la carpeta de entrada.\")\n",
    "        return\n",
    "\n",
    "    # Obtener IDs de pacientes que existen en la carpeta \"Pacientes\"\n",
    "    pacientes_existentes = [p for p in os.listdir(path_pacientes) if os.path.isdir(os.path.join(path_pacientes, p))]\n",
    "\n",
    "    print(f\"üìÇ Pacientes detectados: {pacientes_existentes}\")\n",
    "    print(f\"üìÅ Procesando {len(all_files)} archivos...\\n\")\n",
    "\n",
    "    for filename in all_files:\n",
    "        match = re.match(r\"(\\d+)_([0-9]+)_([0-9]+)\\.txt\", filename)\n",
    "        if not match:\n",
    "            print(f\"‚ùå Formato incorrecto en nombre de archivo: {filename}\")\n",
    "            continue\n",
    "\n",
    "        paciente_id, sesion, campo = match.groups()\n",
    "        paciente_path = os.path.join(path_pacientes, paciente_id)\n",
    "\n",
    "        # Verificar si el paciente existe\n",
    "        if paciente_id not in pacientes_existentes:\n",
    "            print(f\"‚ö†Ô∏è No se encontr√≥ carpeta para el paciente {paciente_id}. Archivo omitido: {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Crear carpeta de sesi√≥n si no existe\n",
    "        sesion_folder = f\"{paciente_id}_{sesion}\"\n",
    "        sesion_path = os.path.join(paciente_path, sesion_folder)\n",
    "        os.makedirs(sesion_path, exist_ok=True)\n",
    "\n",
    "        # Mover o copiar el archivo\n",
    "        src = os.path.join(path_in, filename)\n",
    "        dst = os.path.join(sesion_path, filename)\n",
    "\n",
    "        try:\n",
    "            if move:\n",
    "                shutil.move(src, dst)\n",
    "            else:\n",
    "                shutil.copy2(src, dst)\n",
    "            print(f\"‚úÖ Archivo {filename} ‚Üí {sesion_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error moviendo {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Reorganizaci√≥n completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57036242",
   "metadata": {},
   "source": [
    "---------------  \n",
    "## Funci√≥n `analyze_data(base_path)`\n",
    "\n",
    "**Finalidad:**  \n",
    "Automatiza el an√°lisis completo de todas las curvas respiratorias de m√∫ltiples pacientes y sesiones.  \n",
    "Recorre la carpeta principal `\"Pacientes\"` y, para cada paciente, procesa todas sus sesiones de tratamiento aplicando las funciones desarrolladas previamente:  \n",
    "`import_UM()`, `merge_timelines()`, `cut_delay()`, `detect_groups()`.\n",
    "\n",
    "**Flujo de trabajo:**  \n",
    "1. Busca las carpetas de cada paciente dentro de `base_path`.  \n",
    "2. Para cada paciente:\n",
    "   - Carga el archivo `ID_UM.txt` con la funci√≥n `import_UM()`.\n",
    "   - Itera sobre las carpetas de sesiones (`ID_1`, `ID_2`, ...).  \n",
    "   - En cada sesi√≥n:\n",
    "     - Fusiona los archivos de los distintos campos (`merge_timelines()`).\n",
    "     - Recorta delays iniciales y finales (`cut_delay()`).\n",
    "     - Detecta grupos respiratorios v√°lidos (`detect_groups()`).\n",
    "     - Guarda el dataframe de grupos como `ID_X_groups.txt`.\n",
    "  \n",
    "3. Devuelve un `DataFrame` resumen con la informaci√≥n procesada de todas las sesiones.\n",
    "\n",
    "**Input:**  \n",
    "- `base_path` *(str)* ‚Üí Ruta completa de la carpeta `\"Pacientes\"`.\n",
    "\n",
    "**Output:**  \n",
    "- Devuelve un `DataFrame` con las columnas:\n",
    "  - `Paciente`: ID del paciente.  \n",
    "  - `Sesi√≥n`: Nombre de la carpeta de sesi√≥n.  \n",
    "  - `NumCampos`: N√∫mero de archivos de campos analizados.  \n",
    "  - `NumGrupos`: N√∫mero de grupos respiratorios detectados.  \n",
    "  - `RutaSesion`: Ruta completa de la sesi√≥n.  \n",
    "  - `Resultado`: \"OK\" o descripci√≥n del error si fall√≥ la sesi√≥n.  \n",
    "\n",
    "**Formato de salida generado:**  \n",
    "La funci√≥n crea o actualiza los siguientes archivos en cada carpeta de sesi√≥n:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfc30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(base_path):\n",
    "\n",
    "    resumen = []  # lista de diccionarios para almacenar resultados por sesi√≥n\n",
    "\n",
    "    # Iterar sobre las carpetas de pacientes\n",
    "    for patient_id in sorted(os.listdir(base_path)):\n",
    "        patient_path = os.path.join(base_path, patient_id)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Analizando paciente: {patient_id}\")\n",
    "\n",
    "        um_file = os.path.join(patient_path, f\"{patient_id}_UM.txt\")\n",
    "        if not os.path.exists(um_file):\n",
    "            print(f\"‚ö†Ô∏è No se encontr√≥ {um_file}, se omite este paciente.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_UM = import_UM(um_file)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error leyendo {um_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Iterar sobre las carpetas de sesiones (ID_1, ID_2, ...)\n",
    "        for session in sorted(os.listdir(patient_path)):\n",
    "            session_path = os.path.join(patient_path, session)\n",
    "            if not os.path.isdir(session_path):\n",
    "                continue\n",
    "\n",
    "            print(f\"  ü©∫ Procesando sesi√≥n: {session}\")\n",
    "\n",
    "            # Buscar archivos .txt de campos en la sesi√≥n\n",
    "            field_files = sorted([\n",
    "                os.path.join(session_path, f)\n",
    "                for f in os.listdir(session_path)\n",
    "                if f.endswith(\".txt\") and not f.endswith(\"_UM.txt\")\n",
    "            ])\n",
    "\n",
    "            if not field_files:\n",
    "                print(f\"  ‚ö†Ô∏è No se encontraron archivos de campos en {session_path}\")\n",
    "                resumen.append({\n",
    "                    \"Paciente\": patient_id,\n",
    "                    \"Sesi√≥n\": session,\n",
    "                    \"NumCampos\": 0,\n",
    "                    \"NumGrupos\": None,\n",
    "                    \"RutaSesion\": session_path,\n",
    "                    \"Resultado\": \"Sin campos\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # 1Ô∏è‚É£ Fusionar timelines\n",
    "                df_data = merge_timelines(field_files)\n",
    "\n",
    "                # 2Ô∏è‚É£ Recortar delay\n",
    "                df_data = cut_delay(df_data)\n",
    "\n",
    "                # 3Ô∏è‚É£ Detectar grupos respiratorios\n",
    "                df_groups = detect_groups(df_data, df_UM)\n",
    "\n",
    "                # 4Ô∏è‚É£ Guardar resultados\n",
    "                groups_path = os.path.join(session_path, f\"{session}_processed-data.txt\")\n",
    "                df_groups.to_csv(groups_path, index=False, sep=\"\\t\")\n",
    "\n",
    "                print(f\"  ‚úÖ Sesi√≥n procesada correctamente.\")\n",
    "\n",
    "                resumen.append({\n",
    "                    \"Paciente\": patient_id,\n",
    "                    \"Sesi√≥n\": session,\n",
    "                    \"NumCampos\": len(field_files),\n",
    "                    \"NumGrupos\": len(df_groups),\n",
    "                    \"RutaSesion\": session_path,\n",
    "                    \"Resultado\": \"OK\"\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error procesando {session}: {e}\")\n",
    "                resumen.append({\n",
    "                    \"Paciente\": patient_id,\n",
    "                    \"Sesi√≥n\": session,\n",
    "                    \"NumCampos\": len(field_files),\n",
    "                    \"NumGrupos\": None,\n",
    "                    \"RutaSesion\": session_path,\n",
    "                    \"Resultado\": str(e)\n",
    "                })\n",
    "\n",
    "    # Convertir la lista de resultados en DataFrame resumen\n",
    "    df_resumen = pd.DataFrame(resumen)\n",
    "    print(\"\\nüìã Procesamiento finalizado.\")\n",
    "    #display(df_resumen)\n",
    "\n",
    "    return df_resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a7246",
   "metadata": {},
   "source": [
    "---\n",
    "# 2 - PARAMETER EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdcbfd0",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Funci√≥n `tratar_fechas`\n",
    "\n",
    "**Objetivo:**  \n",
    "Analizar la coherencia temporal de los archivos de campo (`.txt`) de cada paciente, verificando que los campos dentro de cada sesi√≥n y las sesiones dentro de cada paciente est√©n en orden cronol√≥gico correcto.\n",
    "\n",
    "\n",
    "### üìÅ Estructura de carpetas esperada\n",
    "\n",
    "main_folder/\n",
    "‚îÇ\n",
    "\n",
    "‚îú‚îÄ‚îÄ 1234/ ‚Üê Carpeta del paciente (patientID)\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ 1234_1/ ‚Üê Sesi√≥n 1 del paciente\n",
    "\n",
    "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 1234_1_1.txt ‚Üê Campo 1\n",
    "\n",
    "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 1234_1_2.txt ‚Üê Campo 2\n",
    "\n",
    "‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ (otros archivos ignorados)\n",
    "\n",
    "‚îÇ ‚îÇ\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ 1234_2/ ‚Üê Sesi√≥n 2\n",
    "\n",
    "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 1234_2_1.txt\n",
    "\n",
    "‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ 1234_2_2.txt\n",
    "\n",
    "‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "‚îÇ ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "‚îÇ\n",
    "\n",
    "‚îú‚îÄ‚îÄ 5678/\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ 5678_1/\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ 5678_2/\n",
    "\n",
    "‚îÇ ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "\n",
    "- Cada **paciente** tiene una carpeta con su `patientID` como nombre.  \n",
    "- Dentro, cada **sesi√≥n** se nombra como `patientID_sesion` (por ejemplo, `5691706_5`).  \n",
    "- Dentro de cada sesi√≥n, hay varios **campos de tratamiento** con formato `patientID_sesion_campo.txt`.  \n",
    "- Cualquier otro archivo `.txt` que no cumpla ese formato ser√° **ignorado** autom√°ticamente.\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è Funcionamiento\n",
    "\n",
    "1. **Recorre todos los pacientes** dentro del directorio principal (`main_folder`).\n",
    "2. Para cada paciente:\n",
    "   - Analiza todas sus **sesiones**, ordenadas correctamente por n√∫mero (1, 2, 3, ‚Ä¶, 10, 11, ‚Ä¶).\n",
    "   - Dentro de cada sesi√≥n:\n",
    "     - Procesa √∫nicamente los archivos con formato v√°lido `patientID_sesion_campo.txt`.\n",
    "     - Lee la **l√≠nea 12** del archivo (`Started: dd/mm/yyyy. hh:mm:ss`).\n",
    "     - Muestra la l√≠nea le√≠da por pantalla.\n",
    "     - Verifica que los campos est√°n **en orden cronol√≥gico** seg√∫n la hora de inicio.\n",
    "   - Al final de cada sesi√≥n, muestra si los campos est√°n ordenados correctamente.\n",
    "   - Al final de cada paciente, muestra si las sesiones est√°n **en orden cronol√≥gico creciente** (sin exigir que sean d√≠as consecutivos).\n",
    "3. Finalmente, imprime un **resumen general** con:\n",
    "   - Fecha m√°s antigua y m√°s reciente encontradas.\n",
    "   - N√∫mero total de pacientes analizados.\n",
    "   - N√∫mero total de sesiones analizadas.\n",
    "\n",
    "\n",
    "### üßæ Ejemplo de salida por consola\n",
    "\n",
    "ü©∫ Analizando paciente 5691706...\n",
    "\n",
    "‚ûú Sesi√≥n 5:\n",
    "5691706_5_1.txt: Started: 25/01/2025. 09:12:03\n",
    "5691706_5_2.txt: Started: 25/01/2025. 09:19:48\n",
    "‚úÖ Campos en orden cronol√≥gico correcto.\n",
    "\n",
    "‚ûú Sesi√≥n 6:\n",
    "5691706_6_1.txt: Started: 26/01/2025. 09:11:59\n",
    "‚úÖ Campos en orden cronol√≥gico correcto.\n",
    "‚úÖ Sesiones en orden cronol√≥gico correcto.\n",
    "\n",
    "üìÖ Todas las sesiones del paciente 5691706 est√°n en orden cronol√≥gico correcto.\n",
    "\n",
    "üìä Resumen general:\n",
    "\n",
    "Fecha m√°s antigua: 25/01/2025 09:12:03\n",
    "\n",
    "Fecha m√°s reciente: 03/02/2025 10:08:55\n",
    "\n",
    "Total de pacientes: 10\n",
    "\n",
    "Total de sesiones: 145\n",
    "\n",
    "\n",
    "### üß† Notas\n",
    "\n",
    "- No devuelve ning√∫n valor, solo muestra **mensajes informativos** por consola.  \n",
    "- Los errores de lectura o archivos sin la l√≠nea `Started:` se notifican con `‚ö†Ô∏è`.  \n",
    "- Es ideal para **revisi√≥n manual** de la estructura temporal de los datos antes de procesarlos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def tratar_fechas(main_folder):\n",
    "    formato_fecha = \"%d/%m/%Y. %H:%M:%S\"\n",
    "    fechas_globales = []\n",
    "    total_pacientes = 0\n",
    "    total_sesiones = 0\n",
    "\n",
    "    for patient_id in sorted(os.listdir(main_folder)):\n",
    "        patient_path = os.path.join(main_folder, patient_id)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "\n",
    "        total_pacientes += 1\n",
    "        print(f\"\\nü©∫ Analizando paciente {patient_id}...\")\n",
    "\n",
    "        # üîß Orden correcto de las sesiones por n√∫mero\n",
    "        sesiones = sorted(\n",
    "            [\n",
    "                s for s in os.listdir(patient_path)\n",
    "                if os.path.isdir(os.path.join(patient_path, s)) and s.startswith(patient_id + \"_\")\n",
    "            ],\n",
    "            key=lambda x: int(x.split('_')[-1]) if x.split('_')[-1].isdigit() else 0\n",
    "        )\n",
    "\n",
    "        fechas_sesiones = []\n",
    "\n",
    "        for sesion in sesiones:\n",
    "            sesion_path = os.path.join(patient_path, sesion)\n",
    "            print(f\"\\n  ‚ûú Sesi√≥n {sesion.split('_')[-1]}:\")\n",
    "\n",
    "            # ‚úÖ Solo archivos v√°lidos tipo patientID_sesion_campo.txt\n",
    "            campos = sorted([\n",
    "                f for f in os.listdir(sesion_path)\n",
    "                if f.endswith(\".txt\")\n",
    "                and f.startswith(sesion + \"_\")\n",
    "                and len(f.split('_')) == 3  # exactamente tres partes: ID, sesi√≥n, campo\n",
    "                and f.split('_')[-1].replace('.txt', '').isdigit()\n",
    "            ])\n",
    "\n",
    "            fechas_campos = []\n",
    "            for campo in campos:\n",
    "                campo_path = os.path.join(sesion_path, campo)\n",
    "                try:\n",
    "                    with open(campo_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                        lineas = file.readlines()\n",
    "                        if len(lineas) >= 12:\n",
    "                            linea_fecha = lineas[11].strip()  # l√≠nea 12\n",
    "                            if \"Started:\" in linea_fecha:\n",
    "                                fecha_str = linea_fecha.split(\"Started:\")[1].strip()\n",
    "                                fecha = datetime.strptime(fecha_str, formato_fecha)\n",
    "                                fechas_campos.append(fecha)\n",
    "                                print(f\"    {campo}: {linea_fecha}\")  # üïí Mostrar l√≠nea 12\n",
    "                            else:\n",
    "                                print(f\"    ‚ö†Ô∏è {campo}: no se encontr√≥ l√≠nea 'Started:'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ö†Ô∏è Error leyendo {campo}: {e}\")\n",
    "\n",
    "            # Comprobar orden interno de campos\n",
    "            if len(fechas_campos) > 1:\n",
    "                for i in range(1, len(fechas_campos)):\n",
    "                    if fechas_campos[i] < fechas_campos[i - 1]:\n",
    "                        print(f\"    ‚ùå Orden incorrecto en campos ({campos[i-1]} ‚Üí {campos[i]})\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"    ‚úÖ Campos en orden cronol√≥gico correcto.\")\n",
    "            elif fechas_campos:\n",
    "                print(\"    ‚úÖ Solo un campo, no se requiere comprobaci√≥n de orden.\")\n",
    "            else:\n",
    "                print(\"    ‚ö†Ô∏è No se encontraron archivos v√°lidos de campo.\")\n",
    "\n",
    "            if fechas_campos:\n",
    "                fecha_sesion = fechas_campos[0].date()\n",
    "                fechas_sesiones.append(fecha_sesion)\n",
    "                fechas_globales.extend(fechas_campos)\n",
    "\n",
    "        # ‚úÖ Comprobar orden cronol√≥gico entre sesiones (no consecutividad)\n",
    "        if len(fechas_sesiones) > 1:\n",
    "            for i in range(1, len(fechas_sesiones)):\n",
    "                if fechas_sesiones[i] < fechas_sesiones[i - 1]:\n",
    "                    print(f\"  ‚ùå Sesiones desordenadas: {fechas_sesiones[i-1]} ‚Üí {fechas_sesiones[i]}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"  ‚úÖ Sesiones en orden cronol√≥gico correcto.\")\n",
    "        elif fechas_sesiones:\n",
    "            print(\"  ‚úÖ Solo una sesi√≥n, no se requiere comprobaci√≥n de orden.\")\n",
    "\n",
    "        # üßæ Nuevo resumen por paciente\n",
    "        if len(fechas_sesiones) > 1:\n",
    "            if all(fechas_sesiones[i] >= fechas_sesiones[i - 1] for i in range(1, len(fechas_sesiones))):\n",
    "                print(f\"\\n  üìÖ Todas las sesiones del paciente {patient_id} est√°n en orden cronol√≥gico correcto.\")\n",
    "            else:\n",
    "                print(f\"\\n  ‚ö†Ô∏è Algunas sesiones del paciente {patient_id} no est√°n en orden cronol√≥gico.\")\n",
    "        else:\n",
    "            print(f\"\\n  üìÖ Paciente {patient_id} tiene una sola sesi√≥n.\")\n",
    "\n",
    "        total_sesiones += len(fechas_sesiones)\n",
    "\n",
    "    # Resultados globales\n",
    "    if fechas_globales:\n",
    "        fecha_mas_antigua = min(fechas_globales)\n",
    "        fecha_mas_reciente = max(fechas_globales)\n",
    "        print(\"\\nüìä Resumen general:\")\n",
    "        print(f\"   - Fecha m√°s antigua: {fecha_mas_antigua.strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "        print(f\"   - Fecha m√°s reciente: {fecha_mas_reciente.strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "        print(f\"   - Total de pacientes: {total_pacientes}\")\n",
    "        print(f\"   - Total de sesiones: {total_sesiones}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No se encontraron fechas v√°lidas en ning√∫n archivo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7460ef",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Funci√≥n `build_plot`\n",
    "\n",
    "**Objetivo:**  \n",
    "Generar y guardar una representaci√≥n gr√°fica de la curva DIBH a partir de un archivo de datos procesado (`*_processed-data.txt`), mostrando la amplitud del movimiento respiratorio, las activaciones del haz de radiaci√≥n (Beam) y los distintos grupos de tratamiento (Grupo), junto con los valores umbral (`UpperThreshold` y `LowerThreshold`).\n",
    "\n",
    "\n",
    "### üìÑ Entrada esperada\n",
    "\n",
    "Archivo de texto (`.txt`) tabulado con formato:\n",
    "\n",
    "| Time | Amplitude | Beam | Grupo | UpperThreshold | LowerThreshold |\n",
    "|------|------------|------|--------|----------------|----------------|\n",
    "| *meta-row* | ... | ... | ... | **valor** | **valor** |\n",
    "| 0.0 | 0.12 | 0 | 0 | ... | ... |\n",
    "| 0.1 | 0.15 | 0 | 1 | ... | ... |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "- La **primera fila** contiene los nombres de las columnas (header).  \n",
    "- La **segunda fila (meta-row)** contiene los valores de `UpperThreshold` y `LowerThreshold`.  \n",
    "- El resto de filas representan los datos temporales medidos durante el tratamiento.\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è Funcionamiento\n",
    "\n",
    "1. **Lectura del archivo:**\n",
    "   - Se lee el `.txt` mediante `pandas.read_csv` con separador `\\t`.\n",
    "   - Se extraen los valores de los umbrales (`UpperThreshold`, `LowerThreshold`) desde la **meta-row**.\n",
    "   - Se eliminan las filas de cabecera y meta para quedarse solo con los datos reales.\n",
    "\n",
    "2. **Procesamiento de datos:**\n",
    "   - Se convierten las columnas num√©ricas (`Time`, `Amplitude`, `Beam`, `Grupo`) a tipo float.\n",
    "   - Se eliminan filas inv√°lidas (por ejemplo, donde `Time` o `Amplitude` no sean num√©ricos).\n",
    "   - Se asegura que, si alguna columna no existe, se rellena con `NaN` para mantener la estructura del DataFrame.\n",
    "\n",
    "3. **Generaci√≥n del gr√°fico:**\n",
    "   - Se representa:\n",
    "     - `Amplitude` en azul frente al tiempo (`Time`).\n",
    "     - `Beam` en rojo si contiene valores v√°lidos.\n",
    "     - √Åreas coloreadas bajo la curva para cada grupo (`Grupo ‚â† 0`).\n",
    "   - Se a√±aden l√≠neas horizontales para los **umbrales superior e inferior** (`UpperThreshold`, `LowerThreshold`).\n",
    "\n",
    "4. **Est√©tica y guardado:**\n",
    "   - Se configuran t√≠tulos, etiquetas, cuadr√≠cula y leyenda.\n",
    "   - Se guarda el gr√°fico como imagen `.png` con el mismo nombre del archivo de entrada, reemplazando:\n",
    "     ```\n",
    "     *_processed-data.txt  ‚Üí  *_plot.png\n",
    "     ```\n",
    "   - El gr√°fico no se muestra en pantalla (modo no interactivo), pero puede visualizarse activando manualmente `plt.show()`.\n",
    "\n",
    "\n",
    "### üßæ Ejemplo de salida\n",
    "‚úÖ Gr√°fico guardado en: 5691706_5_3_plot.png\n",
    "\n",
    "El gr√°fico muestra:\n",
    "\n",
    "- **Curva azul:** amplitud del movimiento tor√°cico durante la respiraci√≥n.\n",
    "- **Curva roja:** activaciones del haz de radiaci√≥n (Beam).\n",
    "- **√Åreas sombreadas:** intervalos de tratamiento por grupo.\n",
    "- **L√≠neas horizontales:** umbrales superior e inferior.\n",
    "\n",
    "\n",
    "### üß† Notas\n",
    "\n",
    "- La funci√≥n **no devuelve ning√∫n valor**, √∫nicamente genera y guarda la figura.  \n",
    "- Si se desea mostrarla directamente, puede activarse `plt.show()` al final.  \n",
    "- Es compatible con los archivos de salida generados por las funciones de procesamiento previas (`merge_timelines`, `extract_parameters`, etc.).  \n",
    "- Requiere las librer√≠as `pandas`, `numpy`, `matplotlib` e `itertools`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b427d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plot(path):\n",
    "    plt.ioff()  # Desactiva el modo interactivo\n",
    "    \n",
    "    # Leer archivo: la primera fila es header; la siguiente fila es la meta-row con thresholds\n",
    "    df_raw = pd.read_csv(path, sep='\\t', header=0, comment='#', skipinitialspace=True)\n",
    "    \n",
    "    # Extraer thresholds desde la meta-row (primera fila de datos)\n",
    "    # Asumimos que siempre existe la meta-row y que tiene valores en UpperThreshold y LowerThreshold\n",
    "    upper = df_raw.loc[0, 'UpperThreshold']\n",
    "    lower = df_raw.loc[0, 'LowerThreshold']\n",
    "    \n",
    "    # Eliminar la meta-row para quedarse solo con los datos temporales reales\n",
    "    df = df_raw.drop(index=0).reset_index(drop=True)\n",
    "    \n",
    "    # Convertir columnas a num√©rico (Time siempre existe seg√∫n tus datos)\n",
    "    df['Time'] = pd.to_numeric(df['Time'], errors='coerce')\n",
    "    for col in ['Amplitude', 'Beam', 'Grupo']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        else:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Eliminar filas donde Time o Amplitude no sean v√°lidos\n",
    "    df = df.dropna(subset=['Time', 'Amplitude']).reset_index(drop=True)\n",
    "    \n",
    "    # Preparar figura\n",
    "    fig, ax = plt.subplots(figsize=(30,12))\n",
    "    \n",
    "    # Amplitud vs Time (azul)\n",
    "    ax.plot(df['Time'], df['Amplitude'], label='Amplitud', color='blue', linewidth=1.5, zorder=3)\n",
    "    \n",
    "    # Beam vs Time (rojo) si existe/contiene valores\n",
    "    if df['Beam'].notna().any():\n",
    "        ax.plot(df['Time'], df['Beam'], label='Beam', color='red', linewidth=1.2, zorder=2)\n",
    "    else:\n",
    "        # no dibujamos beam si no hay datos\n",
    "        pass\n",
    "    \n",
    "    # Rellenar el area bajo la curva de Amplitud por cada Grupo distinto de 0\n",
    "    if 'Grupo' in df.columns and df['Grupo'].notna().any():\n",
    "        unique_groups = np.unique(df['Grupo'].dropna())\n",
    "        unique_groups = unique_groups[unique_groups != 0]  # eliminar ceros\n",
    "        if len(unique_groups) > 0:\n",
    "            color_cycle = itertools.cycle(['#87CEFA', '#FFA07A', '#98FB98', '#DDA0DD', '#FFD700', '#A9A9A9'])\n",
    "            for g in unique_groups:\n",
    "                mask = df['Grupo'] == g\n",
    "                color = next(color_cycle)\n",
    "                ax.fill_between(df['Time'], df['Amplitude'], where=mask, color=color, alpha=0.35,\n",
    "                                label=f'Grupo {int(g)}', step=None, zorder=1)\n",
    "    \n",
    "    # Dibujar thresholds como l√≠neas horizontales constantes\n",
    "    # (convertimos a float por si vinieran como numpy types)\n",
    "    try:\n",
    "        upper_val = float(upper)\n",
    "        ax.axhline(y=upper_val, color='purple', linestyle='--', linewidth=1, label=f'Upper Threshold ({upper_val:.3f})', zorder=4)\n",
    "    except Exception:\n",
    "        upper_val = None\n",
    "    \n",
    "    try:\n",
    "        lower_val = float(lower)\n",
    "        ax.axhline(y=lower_val, color='orange', linestyle='--', linewidth=1, label=f'Lower Threshold ({lower_val:.3f})', zorder=4)\n",
    "    except Exception:\n",
    "        lower_val = None\n",
    "    \n",
    "    # Est√©tica\n",
    "    ax.set_title('Curva DIBH')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude (cm) / Beam')\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    # Guardar y mostrar\n",
    "    output_path = path.replace('_processed-data.txt', '_plot.png')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #si quieres guardar la figura en el path, esto\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close('all')\n",
    "    \n",
    "    #si quieres imprimir la figura aqui, esto:\n",
    "    #plt.show()\n",
    "    \n",
    "    #print(f\"‚úÖ Gr√°fico guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395166e8",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------\n",
    "# EJECUTABLES\n",
    "\n",
    "---\n",
    "\n",
    "### Flujo Ideal del codigo completo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3cc10",
   "metadata": {},
   "source": [
    "---\n",
    "-Llamar funci√≥n **IMPORT_QUERIES(file_path,output_dir)**:\n",
    "\n",
    "\tEsta funci√≥n lee un \"queries.out\" y crea en \"output_dir\" carpetas para cada \n",
    "\t\"patientID\" con un documento \"patientID_UM.txt\" en el que estan las UM de \n",
    "\tcada campo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9341f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_queries = r\"C:\\Users\\Mario\\Desktop\\Fisica\\TFG Clinic\\TFG\\DIBH\\queries_DIBH.out\"\n",
    "output_dir = r\"C:\\Users\\Mario\\Desktop\\Fisica\\TFG Clinic\\TFG\\DIBH\\Pacientes_NO_entreno\"\n",
    "res = import_queries(path_queries, output_dir) #NO OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5e007",
   "metadata": {},
   "source": [
    "---\n",
    "-Llamar funci√≥n **READ_PATIENTS(path_in,path_pacientes)**:\n",
    "\n",
    "\tEsta funci√≥n analiza una carpeta \"path_in\" en la que estan los txt de\n",
    "\tlas M sesi√≥nes de los N pacientes, genera M carpetas en el \"path_pacientes\"\n",
    "\tdentro de las N carpetas creadas anteriormente con IMPORT_QUERIES y ordena \n",
    "\ttodos los txt en sus correspondientes carpetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in=r\"C:\\Users\\Mario\\Desktop\\Fisica\\TFG Clinic\\TFG\\DIBH\\ESTUDI DIBH\"\n",
    "path_pacientes=r\"C:\\Users\\Mario\\Desktop\\Fisica\\TFG Clinic\\TFG\\DIBH\\Pacientes_NO_entreno\"\n",
    "read_patients(path_in,path_pacientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49303693",
   "metadata": {},
   "source": [
    "---\n",
    "-Llamar funci√≥n **TREAT_DATES(base_path)**:\n",
    "\n",
    "\tEsta funci√≥n recibe un \"base_path\", que es el directorio donde estan las N \n",
    "\tcarpetas de pacientes. La idea de esta funci√≥n es que repase las fechas de \n",
    "\tlos archivos camp para comprobar que cada camp esta en su carpeta sesion\n",
    "\tcorrecta. Tambien repasa cada carpeta sesion para comprobar que los archivos\n",
    "\tcamp estan correctamente ordenados (segun la hora de inicio de cada camp,\n",
    "\tespecificada en el archivo al lado de la fecha).\n",
    "\tUnicamente hace print de OK o no OK si esta cada archivo donde debe o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = r\"C:\\Users\\Mario\\Desktop\\Fisica\\TFG Clinic\\TFG\\DIBH\\Pacientes_NO_entreno\"\n",
    "tratar_fechas(main_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16143da0",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "Hasta aqui tenemos N carpetas de pacientes, dentro de cada una hay M sesiones y eldocumento de las UM, y dentro de cada carpeta de sesion estan los campos de treatment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63261a4e",
   "metadata": {},
   "source": [
    "---\n",
    "-Llamar funci√≥n **ANALYZE_DATA(base_path)**:\n",
    "\n",
    "\tEsta funci√≥n recibe el mismo \"base_path\" que treat_dates. El objetivo de esta \n",
    "\tfunci√≥n es analizar cada carpeta paciente, cada subcarpeta sesi√≥n y hacer el\n",
    "\t\"merge_timelines\" de los txt de treatment y dar de vuelta un \n",
    "\t\"patientID_sesion_processed-data.txt\" que corresponde al dataframe que da la \n",
    "\tfunci√≥n merge_timelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47aa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_data(r\"C:\\Users\\Mario\\Desktop\\Fisica\\TFG Clinic\\Mario\\Documentaci√≥ inicial\\DIBH_superposat\\Pacientes_NO_entreno\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
